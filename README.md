
## Cases for Explainable Software Systems: Characteristics and Examples
A comprehensive understanding of what needs explanation is the primary step to make any system explainable. Nevertheless, only a little attention has been paid towards the characterization of system behavior that confuses users and requires explanations <sup> [[1]](#1).
 
In this repository we present a [taxonomy](taxonomy.md) that structures needs for an explanation according to different reasons. For each leaf node in the taxonomy, we provide a scenario that describes a concrete situation in which a software system should provide an explanation. These scenarios, called explanation cases, illustrate the different demands for explanations.

Our taxonomy and the explanation cases can already be used to guide the requirements elicitation for explanation capabilities of interactive intelligent systems. By following the taxonomy, interaction scenarios of the system, even rare cases, can be analyzed to identify necessary explanations and the influence factors can help to further decide on when and how to explain.  

We are convinced that our findings will also help the community  to align future research on explainable systems by providing precise descriptions of explanation needs and a benchmark of explanation cases.

##### Authors:
_Mersedeh Sadeghi, Verena Kl√∂s and Andreas Vogelsang_


##### References
<sup> <a id="1">[1]</a>: Nunes, Ingrid, and Dietmar Jannach. "A systematic review and taxonomy of explanations in decision support and recommender systems." User Modeling and User-Adapted Interaction 27, no. 3 (2017): 393-444.